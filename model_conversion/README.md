# LOOM Model Conversion & Inference

This directory contains tools for running LOOM transformer models with a **pure Go stack** (no Python dependencies).

## Quick Start

### 1. Download a model

Using HuggingFace CLI:

```bash
pip install -U huggingface_hub
huggingface-cli download Qwen/Qwen2.5-0.5B
```

Or using Python:

```bash
python3 -c "from huggingface_hub import snapshot_download; snapshot_download('Qwen/Qwen2.5-0.5B')"
```

The model will be cached in `~/.cache/huggingface/hub/`

### 2. Start the backend server

**Terminal 1:**

```bash
cd model_conversion
go run serve_model_bytes.go -model Qwen/Qwen2.5-0.5B -port 8080
```

### 3. Start the web interface

**Terminal 2:**

```bash
cd model_conversion
go run web_interface.go -model Qwen/Qwen2.5-0.5B -backend http://localhost:8080 -port 5000
```

### 4. Open your browser

Navigate to: **http://localhost:5000**

## Supported Models (â‰¤1B Parameters)

**âš ï¸ Note: CPU inference is currently VERY SLOW. Performance will be significantly improved with upcoming WebGPU acceleration.**

All models under ~1B parameters should work. Here are tested configurations:

### Qwen Models (Alibaba)

```bash
# Qwen2.5-0.5B (494M) - Fast, great quality
huggingface-cli download Qwen/Qwen2.5-0.5B
go run serve_model_bytes.go -model Qwen/Qwen2.5-0.5B -port 8080
go run web_interface.go -model Qwen/Qwen2.5-0.5B -backend http://localhost:8080 -port 5000

# Qwen2.5-0.5B-Instruct (494M) - Instruction-tuned
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct
go run serve_model_bytes.go -model Qwen/Qwen2.5-0.5B-Instruct -port 8080
go run web_interface.go -model Qwen/Qwen2.5-0.5B-Instruct -backend http://localhost:8080 -port 5000
```

### SmolLM Models (HuggingFace)

```bash
# SmolLM2-360M-Instruct (360M) - Very fast, excellent for testing
huggingface-cli download HuggingFaceTB/SmolLM2-360M-Instruct
go run serve_model_bytes.go -model HuggingFaceTB/SmolLM2-360M-Instruct -port 8080
go run web_interface.go -model HuggingFaceTB/SmolLM2-360M-Instruct -backend http://localhost:8080 -port 5000

# SmolLM2-1.7B-Instruct (1.7B) - Larger, better quality (slower on CPU)
huggingface-cli download HuggingFaceTB/SmolLM2-1.7B-Instruct
go run serve_model_bytes.go -model HuggingFaceTB/SmolLM2-1.7B-Instruct -port 8080
go run web_interface.go -model HuggingFaceTB/SmolLM2-1.7B-Instruct -backend http://localhost:8080 -port 5000
```

### TinyLlama Models (Meta/Community)

```bash
# TinyLlama-1.1B-Chat (1.1B) - Popular, well-trained
huggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v1.0
go run serve_model_bytes.go -model TinyLlama/TinyLlama-1.1B-Chat-v1.0 -port 8080
go run web_interface.go -model TinyLlama/TinyLlama-1.1B-Chat-v1.0 -backend http://localhost:8080 -port 5000
```

### Pythia Models (EleutherAI)

```bash
# Pythia-410M (410M) - GPT-NeoX architecture
huggingface-cli download EleutherAI/pythia-410m
go run serve_model_bytes.go -model EleutherAI/pythia-410m -port 8080
go run web_interface.go -model EleutherAI/pythia-410m -backend http://localhost:8080 -port 5000

# Pythia-1B (1B)
huggingface-cli download EleutherAI/pythia-1b
go run serve_model_bytes.go -model EleutherAI/pythia-1b -port 8080
go run web_interface.go -model EleutherAI/pythia-1b -backend http://localhost:8080 -port 5000
```

### OPT Models (Meta)

```bash
# OPT-350M (350M) - Original Meta model
huggingface-cli download facebook/opt-350m
go run serve_model_bytes.go -model facebook/opt-350m -port 8080
go run web_interface.go -model facebook/opt-350m -backend http://localhost:8080 -port 5000

# OPT-1.3B (1.3B)
huggingface-cli download facebook/opt-1.3b
go run serve_model_bytes.go -model facebook/opt-1.3b -port 8080
go run web_interface.go -model facebook/opt-1.3b -backend http://localhost:8080 -port 5000
```

## Model Conversion (Fixing Legacy Models)

Some models (like older T5 or PyTorch-only checkpoints) may cause errors due to missing `config.json`, missing `tokenizer.json`, or shared tensors.

We provide a conversion script to fix these models automatically:

1.  **Install dependencies:**
    ```bash
    pip install transformers huggingface-hub safetensors
    ```

2.  **Run conversion script:**
    ```bash
    # Example: Converting the ARC-AGI model SofiTesfay2010/HRM-LLM
    python3 convert_hf_model.py SofiTesfay2010/HRM-LLM
    ```

    This will:
    - Download the model if needed
    - Generate correct `config.json`
    - Convert weights to `model.safetensors`
    - Handle shared tensors (e.g. for T5)
    - Save files directly to your HuggingFace cache

3.  **Run the model:**
    ```bash
    go run quick_talk.go
    # Select the model from the list
    ```

### Performance Expectations (CPU)

| Model Size | Tokens/sec (CPU) | Tokens/sec (WebGPU - Coming Soon) |
| ---------- | ---------------- | --------------------------------- |
| 360M       | ~2-4 tok/s       | ~50-100 tok/s (estimated)         |
| 500M       | ~1-3 tok/s       | ~40-80 tok/s (estimated)          |
| 1.1B       | ~0.5-1.5 tok/s   | ~20-40 tok/s (estimated)          |

**WebGPU acceleration is in development and will provide 10-50x speedup!**

## Model Servers

### serve_model_bytes.go â­ RECOMMENDED

**Pure Go inference server** - Loads entire model into memory using `nn.LoadTransformerFromBytes()`

- Reads config.json and model.safetensors into byte slices
- Uses the nn framework's native transformer loading
- Automatic HuggingFace cache path resolution
- Cleaner code, easier to maintain
- HTTP API with streaming support

```bash
go run serve_model_bytes.go -model Qwen/Qwen2.5-0.5B -port 8080
```

### web_interface.go â­ RECOMMENDED

**Pure Go web frontend** - Replaces Python Flask with native Go HTTP server

- Pure Go BPE tokenizer (no Python dependencies)
- Embedded HTML templates
- Real-time streaming generation
- Connects to any backend server

```bash
go run web_interface.go -model Qwen/Qwen2.5-0.5B -backend http://localhost:8080 -port 5000
```

### serve_model_auto.go (Legacy)

Automatically loads models with dynamic EOS token detection

- Reads safetensors directly from disk
- Flexible tensor key matching
- Good for quick testing
- **Note:** Consider using `serve_model_bytes.go` instead

### serve_model_manual.go (Legacy)

Manual model construction with explicit layer-by-layer loading

- Most control over model structure
- Useful for debugging
- **Note:** Consider using `serve_model_bytes.go` instead

### web_interface.py (Deprecated)

Python Flask web interface - replaced by `web_interface.go`

- **Deprecated:** Use pure Go `web_interface.go` instead
- Requires Python + transformers library (2GB+ dependencies)
- Can be removed from your system

## Architecture

### Pure Go Stack ğŸ¯

The recommended setup uses **zero Python dependencies**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Browser (http://localhost:5000)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  web_interface.go  â”‚ â† Pure Go BPE Tokenizer
         â”‚   (Port 5000)      â”‚   Embedded HTML templates
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ HTTP
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ serve_model_bytes  â”‚ â† nn.LoadTransformerFromBytes
         â”‚   (Port 8080)      â”‚   ForwardCPU inference
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**

- âœ… No Python runtime required
- âœ… No transformers library (saves 2GB+)
- âœ… Single binary deployment
- âœ… Consistent performance
- âœ… Easy to cross-compile

## How It Works

1. **Backend Server** (`serve_model_bytes.go`):

   - Resolves model name to HuggingFace cache path
   - Loads config.json and model.safetensors into memory
   - Uses `nn.LoadTransformerFromBytes()` for initialization
   - Provides HTTP API on port 8080
   - Supports streaming generation (Server-Sent Events)
   - Uses `Network.ForwardCPU` for inference

2. **Web Interface** (`web_interface.go`):

   - Pure Go HTTP server on port 5000
   - Loads tokenizer.json into memory
   - Uses pure Go BPE tokenizer (supports GPT-2, LLaMA, Qwen, Mistral, T5, etc.)
   - Tokenizes input prompts
   - Streams tokens in real-time from backend
   - Serves embedded HTML templates

3. **Streaming Flow**:
   ```
   Browser â†’ Go Web â†’ Go Backend â†’ Go Web â†’ Browser
   (EventSource)   (HTTP+SSE)         (SSE)
   ```

## API Endpoints

### Backend Server (Port 8080)

**POST /generate** - Generate text

Request:

```json
{
  "input_ids": [12522, 5193, 264, 882],
  "max_new_tokens": 50,
  "temperature": 0.7,
  "stream": true // Enable streaming
}
```

Response (streaming):

```
data: {"token": 151643, "done": false}
data: {"token": 8256, "done": false}
data: {"done": true}
```

Response (non-streaming):

```json
{
  "output_ids": [12522, 5193, 264, 882, 151643, 8256, ...]
}
```

**GET /health** - Health check

Response:

```json
{
  "status": "ok",
  "model": "Qwen/Qwen2.5-0.5B",
  "vocab_size": 151936,
  "hidden_size": 896
}
```

### Web Interface (Port 5000)

**GET /** - Web UI (HTML page)

**POST /generate** - Generate text (non-streaming)

Request:

```json
{
  "prompt": "Once upon a time",
  "max_tokens": 50
}
```

Response:

```json
{
  "generated_text": "Once upon a time, there was a...",
  "num_tokens": 15
}
```

**POST /generate_stream** - Stream generation (Server-Sent Events)

**GET /health** - Health check

Response:

```json
{
  "web_interface": "ok",
  "backend": "ok",
  "model": "Qwen/Qwen2.5-0.5B",
  "tokenizer": "Pure Go BPE (vocab: 151936)"
}
```

## Requirements

### Pure Go Stack (Recommended)

**No Python required!** Just Go 1.21+:

```bash
go version  # Should be 1.21 or higher
```

### Legacy Python Stack (Deprecated)

If you still want to use `web_interface.py`:

```bash
pip install flask transformers requests
```

**Note:** The Python stack adds ~2GB of dependencies and is no longer maintained.

## Troubleshooting

**Backend won't start:**

- Verify model is downloaded: `ls ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/`
- Check for errors in terminal output
- Ensure port 8080 is available

**Web interface can't connect to backend:**

- Verify backend is running: `curl http://localhost:8080/health`
- Check backend URL matches: `-backend http://localhost:8080`
- Ensure firewall allows local connections

**Model not found:**

- Download the model first (see Quick Start step 1)
- Or specify full path: `-model ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/xxx`

**Tokenizer errors:**

- Ensure model has `tokenizer.json` file
- Check model is compatible (GPT-2 style BPE, not WordPiece/Unigram)

**Slow generation:**

- Normal for CPU inference (~1-3 tokens/sec on small models)
- Larger models (7B+) are very slow on CPU
- GPU acceleration coming soon

**Garbled text output:**

- Should now be fixed! Text is cleaned automatically
- Supports both `Ä ` (GPT-2, LLaMA, Qwen) and `â–` (T5, SentencePiece) space encoding

## Technical Details

### Pure Go BPE Tokenizer

Located in `/tokenizer/bpe.go`, this is a from-scratch implementation supporting:

- **HuggingFace tokenizer.json format**
- **BPE merging algorithm** with proper pair ranking
- **Pre-tokenization** using regex patterns
- **Byte fallback** for unknown characters
- **Special tokens** handling
- **Universal space encoding:**
  - `Ä ` (U+0120) â†’ space (GPT-2, GPT-3, GPT-4, LLaMA, Qwen, Mistral, Phi)
  - `â–` (U+2581) â†’ space (T5, XLNet, SentencePiece)
  - `<0xHH>` â†’ byte tokens

**Supports models:**

- âœ… GPT-2, GPT-3, GPT-4
- âœ… LLaMA, LLaMA 2, LLaMA 3
- âœ… Qwen, Qwen 2, Qwen 2.5
- âœ… Mistral, Mixtral
- âœ… Phi, Phi-2, Phi-3
- âœ… CodeLlama, StarCoder
- âœ… T5, mT5 (with â– encoding)

### Bytes-Based Loading Pattern

All loaders follow a consistent pattern for flexibility:

```go
// Tokenizer
tokenizer.LoadFromBytes(data []byte)
tokenizer.LoadFromFile(path string)  // wrapper

// Safetensors
nn.LoadSafetensorsFromBytes(data []byte)
nn.LoadSafetensors(path string)  // wrapper

// Transformer
nn.LoadTransformerFromBytes(configData, weightsData []byte)
nn.LoadTransformerFromSafetensors(modelDir string)  // wrapper
```

**Benefits:**

- Enables embedding models in binaries
- Supports streaming from network
- Easier testing with in-memory data
- Clean separation of I/O and parsing

### Fixed SwiGLU Bug

The original implementation had incorrect weight indexing for transposed weight matrices. The fix changes from row-major to column-major indexing:

**Before (wrong):**

```go
gateWeights[i*inputSize+j]  // Row-major for [intermediate][input]
```

**After (correct):**

```go
gateWeights[j*intermediateSize+i]  // Column-major for transposed [input][intermediate]
```

This affects Gate, Up, and Down projections in the SwiGLU MLP layers.

### Model Path Resolution

Short model names are automatically resolved to HuggingFace cache:

```go
"Qwen/Qwen2.5-0.5B" â†’ "~/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B/snapshots/xxx/"
```

Full paths also work:

```go
"~/models/Qwen2.5-0.5B" â†’ "/home/user/models/Qwen2.5-0.5B/"
```

### Validation

Use `trace_all_layers_loom.go` to validate layer outputs against PyTorch:

```bash
cd model_conversion
go run trace_all_layers_loom.go
```

Should show all layers matching within 1e-5 tolerance.
